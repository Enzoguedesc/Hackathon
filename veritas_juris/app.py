import streamlit as st
from dotenv import load_dotenv
import os
import time # For simulating delays if needed in mock functions

# --- Importar as fun√ß√µes do pipeline RAG ---
# Ensure rag_pipeline.py is in the same directory or PYTHONPATH
# and all these functions are correctly defined.
from rag_pipeline import (
    configure_llm,
    initialize_embedding_model,
    load_processes_from_original_json,
    process_documents_for_rag,
    create_vector_store,
    retrieve_relevant_chunks,
    generate_response_with_llm,
    mock_ai_analysis, # Make sure this is correctly defined
    mock_generate_argument_variations # Make sure this is correctly defined
)

# --- 1. Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="VeritasJuris IA Pro",
    page_icon="veritas_juris/logo.png", # Changed icon for thematic relevance
    layout="wide",  
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'mailto:suporte@veritasjuris.ai', # Example
        'Report a bug': "mailto:bugs@veritasjuris.ai", # Example
        'About': """
         ## VeritasJuris IA Pro ‚öñÔ∏è

         **Vers√£o:** 1.1 (Hackathon Edition Melhorada)

         **Desenvolvido para:** Hackathon IBMEC - Desafio de IA Generativa.

         ---

         **Objetivo:** Utilizar Intelig√™ncia Artificial Generativa para realizar an√°lises avan√ßadas e insights sobre jurisprud√™ncia, com foco inicial em Direito Tribut√°rio.

         **Funcionalidades Principais:**
         * An√°lise sem√¢ntica de textos jur√≠dicos.
         * Extra√ß√£o de resumos, palavras-chave e *ratio decidendi*.
         * "Tradu√ß√£o" de termos jur√≠dicos para linguagem clara.
         * Gera√ß√£o de varia√ß√µes argumentativas.
         * Busca inteligente em base de jurisprud√™ncia.

         **Powered by Streamlit & Google Generative AI**
         """
    }
)

# --- T√≠tulo Principal e Logo ---
col_title_logo, col_title_text = st.columns([1, 6])
with col_title_logo:
    # Replace with your actual logo URL or local path if preferred
    st.image("veritas_juris/logo.png", width=150) # Example logo
with col_title_text:
    st.title("VeritasJuris IA Pro")
    st.caption("Seu Assistente Jur√≠dico Inteligente para An√°lise Avan√ßada de Jurisprud√™ncia")

# --- Gerenciamento de Estado da Aplica√ß√£o e Inicializa√ß√£o ---
if 'system_ready' not in st.session_state:
    st.session_state.system_ready = False
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = None
if 'llm_model' not in st.session_state:
    st.session_state.llm_model = None
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'all_chunks_ref' not in st.session_state:
    st.session_state.all_chunks_ref = []
if 'initial_documents' not in st.session_state:
    st.session_state.initial_documents = []

# --- Fun√ß√µes de Cache e Inicializa√ß√£o ---
@st.cache_resource
def load_models_cached():
    # Improved spinner message
    with st.spinner("üîÑ Carregando e configurando modelos de IA (Embedding e LLM)... Por favor, aguarde."):
        embedding_model = initialize_embedding_model()
        llm_model = configure_llm()
    return embedding_model, llm_model

@st.cache_data # Changed to cache_data for data loading
def get_initial_documents_cached(data_file_path):
    if not os.path.exists(data_file_path):
        st.error(f"‚ùå Arquivo JSON principal n√£o encontrado em: {data_file_path}")
        return []
    # Improved spinner message
    with st.spinner(f"üìö Carregando documentos base de '{os.path.basename(data_file_path)}'..."):
        docs = load_processes_from_original_json(data_file_path)
    return docs

@st.cache_resource
def prepare_rag_components_cached(_embedding_model, initial_documents):
    if not initial_documents:
        st.warning("‚ö†Ô∏è Nenhum documento inicial para processar. O √≠ndice RAG n√£o ser√° criado.")
        return None, []
    # Improved spinner message
    with st.spinner("‚öôÔ∏è Processando documentos e construindo o √≠ndice vetorial para RAG..."):
        all_chunks = process_documents_for_rag(initial_documents)
        if not all_chunks:
            st.warning("‚ö†Ô∏è Nenhum chunk gerado a partir dos documentos. O √≠ndice RAG n√£o ser√° criado.")
            return None, []
        index, chunks_with_ids = create_vector_store(all_chunks, _embedding_model)
    return index, chunks_with_ids

def initialize_system():
    """Handles the initialization of models and RAG components."""
    try:
        st.session_state.embedding_model, st.session_state.llm_model = load_models_cached()
        # Ensure the path to your data file is correct
        data_file_path = os.path.join(os.path.dirname(__file__), "data", "processo.json")
        st.session_state.initial_documents = get_initial_documents_cached(data_file_path)

        if st.session_state.initial_documents:
            st.session_state.vector_store, st.session_state.all_chunks_ref = prepare_rag_components_cached(
                st.session_state.embedding_model, st.session_state.initial_documents
            )
            if st.session_state.vector_store and st.session_state.all_chunks_ref:
                st.session_state.system_ready = True
                # Using toast for a less intrusive "ready" message
                # st.toast("‚úÖ Sistema RAG pronto para consultas!", icon="üöÄ") # Moved to after status
            else:
                st.error("‚ö†Ô∏è Falha ao preparar componentes RAG. O √≠ndice vetorial ou os chunks podem n√£o ter sido criados.")
                st.session_state.system_ready = False
        else:
            st.error("‚ö†Ô∏è Documentos iniciais n√£o foram carregados. O sistema RAG n√£o pode ser inicializado.")
            st.session_state.system_ready = False

    except Exception as e:
        st.error(f"‚ùå Erro cr√≠tico durante a inicializa√ß√£o do sistema: {e}")
        st.exception(e) # Provides traceback for debugging
        st.session_state.system_ready = False
# --- Carregar Vari√°veis de Ambiente e Inicializar o Sistema ---
load_dotenv()




status_placeholder = st.empty()

if not st.session_state.get("system_ready", False):
    with status_placeholder.container():
        with st.spinner("‚öôÔ∏è Carregando o sistema para pesquisa..."):
            initialize_system()
        if st.session_state.system_ready:
            with st.status("‚úÖ Carregando o sistema para pesquisa", expanded=False) as status_bar:
                status_bar.update(label="‚úÖ Sistema pronto!", state="complete", expanded=False)
                time.sleep(1)
                status_placeholder.empty()  # limpa o status da tela
        else:
            with st.status("‚ö†Ô∏è Falha na Inicializa√ß√£o. Verifique os erros acima.", expanded=True) as status_bar:
                status_bar.update(state="error")
else:
    status_placeholder.empty()

    
# --- ABAS PARA ORGANIZAR AS FUNCIONALIDADES ---
# Moved RAG query to the first tab as it's a primary feature
tab_rag, tab_analysis, tab_thesis, tab_advanced = st.tabs([
    "üí¨ Consulta √† Base (RAG)",
    "üìù An√°lise de Texto Avulso",
    "üí° Explorador de Teses",
    "‚öôÔ∏è Avan√ßado & Exemplos"
])

with tab_rag:
    st.markdown("<h2 style='font-size: 32px;'>üí¨ Consulta Inteligente √† Base de Jurisprud√™ncia</h2>", unsafe_allow_html=True)
    st.markdown("<p style='font-size: 20px;'>Interaja com o acervo de jurisprud√™ncia carregado fazendo perguntas em linguagem natural.</p>", unsafe_allow_html=True)

    # Label customizada acima da text_area
    st.markdown("<label style='font-size: 18px; font-weight: bold;'>Digite sua pergunta sobre a jurisprud√™ncia carregada:</label>", unsafe_allow_html=True)

    query = st.text_area(
        label="",  # Remove o label nativo para usar o acima
        placeholder="Ex: Quais as implica√ß√µes do RE XXXXX para a cobran√ßa de ICMS sobre Y?",
        key="rag_query_input",
    )

    submit_rag_button = st.button(
        "‚öñÔ∏è Buscar e Responder",
        type="primary",
        key="rag_submit",
        disabled=not st.session_state.system_ready,
        use_container_width=True
    )

    if not st.session_state.system_ready:
        st.warning("üî¥ O Sistema de consulta √† base (RAG) n√£o est√° pronto ou falhou na inicializa√ß√£o. Funcionalidade indispon√≠vel.")

    if submit_rag_button and st.session_state.system_ready:
        if not query:
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta para iniciar a busca.")
        else:
            with st.spinner("üß† Analisando sua pergunta e buscando respostas na base de dados..."):
                try:
                    relevant_chunks_data = retrieve_relevant_chunks(
                        query, st.session_state.vector_store, st.session_state.all_chunks_ref,
                        st.session_state.embedding_model, top_k=5
                    )
                    if not relevant_chunks_data:
                        # Provide feedback if no specific chunks are found
                        st.info("‚ÑπÔ∏è N√£o foram encontrados trechos altamente espec√≠ficos para sua pergunta na base atual. A IA tentar√° fornecer uma resposta mais geral com base no conhecimento dispon√≠vel.")

                    ai_response = generate_response_with_llm(query, relevant_chunks_data, st.session_state.llm_model)

                    st.subheader("üí¨ Resposta do VeritasJuris IA:")
                    st.markdown(ai_response) # Assuming markdown formatted response

                    if relevant_chunks_data:
                        st.divider()
                        st.subheader("üìú Documentos de Refer√™ncia Consultados:")
                        # Use a set to avoid listing the same source document multiple times if different chunks came from it
                        processed_sources = set()
                        for chunk in relevant_chunks_data:
                            source_file = chunk["metadata_chunk"]["source_document"]
                            if source_file not in processed_sources:
                                original_doc = next((doc for doc in st.session_state.initial_documents if doc['source'] == source_file), None)
                                if original_doc:
                                    ementa = original_doc.get('ementa_display_text', 'Ementa n√£o dispon√≠vel.')
                                    doc_id = original_doc.get('id', 'ID n√£o dispon√≠vel')
                                    # Use an expander for each source for cleaner display
                                    with st.expander(f"üìÑ Fonte: {source_file} (ID do Documento: {doc_id})"):
                                        st.markdown(f"**Ementa:**\n {ementa}")
                                        # Optionally, display a snippet of the relevant chunk text
                                        # st.caption(f"Trecho Relevante do Chunk:\n...{chunk['text_chunk'][-200:]}...")
                                else:
                                    st.caption(f"‚ö†Ô∏è Detalhes do documento original n√£o encontrados para: {source_file}")
                                processed_sources.add(source_file)
                except Exception as e:
                    st.error(f"‚ùå Erro ao processar a pergunta RAG: {e}")
                    st.exception(e) # Good for debugging



st.divider()
st.caption(f"VeritasJuris IA Pro v1.1 ‚ú® | Hackathon IBMEC | Streamlit v{st.__version__}")